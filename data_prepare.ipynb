{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/shiyihe/Desktop/USABILITY_ANALYSIS')   \n",
    "\n",
    "from duration_processor import (\n",
    "    load_quiz_data,\n",
    "    extract_format_time,\n",
    "    sanitize_task_names,\n",
    "    summarize_participant_by_format,\n",
    "    plot_participant_time_by_format,\n",
    "    summarize_tasks_by_format,\n",
    "    plot_tasks_time_by_format,\n",
    "    filter_tasks_by_list,\n",
    "    participant_format_statistics,\n",
    "    diagnose_all_json\n",
    ")\n",
    "\n",
    "\n",
    "# load data , rename task name, etc,.\n",
    "folder = '/Users/shiyihe/Desktop/USABILITY_ANALYSIS/tabular'\n",
    "all_data = load_quiz_data(folder, ignore_completed=True)\n",
    "\n",
    "# extract format and duration\n",
    "# df_participant: summarize participant's total time spent by format\n",
    "df_task, df_participant = extract_format_time(all_data)\n",
    "\n",
    "# clean 'format' from task name\n",
    "df_task_clean = sanitize_task_names(df_task)\n",
    "\n",
    "# average total time spent by format\n",
    "df_part_summary = summarize_participant_by_format(df_participant)\n",
    "fig1 = plot_participant_time_by_format(df_part_summary)\n",
    "\n",
    "# 1. all tasks average time spent by format\n",
    "task_list=['reading-task-tabular-1', 'reading-task-tabular-2', 'reading-task-tabular-3']\n",
    "df_filtered = filter_tasks_by_list(df_task_clean, task_list)\n",
    "\n",
    "df_overall = summarize_tasks_by_format(\n",
    "    df_task_clean,\n",
    "    metric='duration_min',\n",
    "    task_list=task_list,\n",
    "    mode='overall'\n",
    ")\n",
    "fig2 = plot_tasks_time_by_format(\n",
    "    df_overall,\n",
    "    metric='average_duration_min',\n",
    "    mode='overall',\n",
    "    title='Overall Avg Time of Selected Tasks by Format',\n",
    "    xlabel='Data Format',\n",
    "    ylabel='Avg Minutes'\n",
    ")\n",
    "\n",
    "# 2. each task average time spent by format\n",
    "df_by_task = summarize_tasks_by_format(\n",
    "    df_task_clean,\n",
    "    task_list=task_list,\n",
    "    metric='duration_min',\n",
    "    mode='by_task'\n",
    ")\n",
    "fig3 = plot_tasks_time_by_format(\n",
    "    df_by_task,\n",
    "    metric='average_duration_min',\n",
    "    mode='by_task',\n",
    "    title='Avg Time per Task by Format',\n",
    "    xlabel='Data Format',\n",
    "    ylabel='Avg Minutes'\n",
    ")\n",
    "\n",
    "# df_task\n",
    "# df_participant\n",
    "# loaded, errors = diagnose_all_json(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "format_counts = participant_format_statistics(df_participant)\n",
    "print(format_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import levene, f_oneway, kruskal\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_duration_by_format(\n",
    "    df: pd.DataFrame,\n",
    "    duration_col: str = 'total_duration_min',\n",
    "    factor_col: str = 'format'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Perform Levene's test for homogeneity of variances, followed by either\n",
    "    one-way ANOVA or Kruskal–Wallis test on `duration_col` grouped by `factor_col`.\n",
    "    Returns a dictionary containing:\n",
    "      - 'levene': (statistic, p-value)\n",
    "      - 'method': 'ANOVA' or 'Kruskal–Wallis'\n",
    "      - 'stat': F-statistic or H-statistic\n",
    "      - 'p_value': p-value of the chosen test\n",
    "      - 'tukey': TukeyHSDResults object if ANOVA was used, else None\n",
    "      - 'anova_table': pandas DataFrame with Type II ANOVA table from statsmodels\n",
    "    \"\"\"\n",
    "    # Prepare a list of arrays, one per group\n",
    "    grouped = df.groupby(factor_col)\n",
    "    groups = [group[duration_col].values for _, group in grouped]\n",
    "    if len(groups) < 2:\n",
    "        raise ValueError(\"At least two groups are required for the comparison.\")\n",
    "\n",
    "    # 1) Levene's test for homogeneity of variances\n",
    "    w_stat, p_levene = levene(*groups)\n",
    "\n",
    "    # 2) Choose between ANOVA and Kruskal–Wallis based on Levene's result\n",
    "    if p_levene > 0.05:\n",
    "        stat, pval = f_oneway(*groups)\n",
    "        method = 'ANOVA'\n",
    "    else:\n",
    "        stat, pval = kruskal(*groups)\n",
    "        method = 'Kruskal–Wallis'\n",
    "\n",
    "    # 3) If ANOVA was used, perform Tukey HSD post-hoc comparisons\n",
    "    tukey = None\n",
    "    if method == 'ANOVA':\n",
    "        tukey = pairwise_tukeyhsd(\n",
    "            endog=df[duration_col],\n",
    "            groups=df[factor_col],\n",
    "            alpha=0.05\n",
    "        )\n",
    "\n",
    "    # 4) Generate a Type II ANOVA table with statsmodels\n",
    "    model = ols(f'{duration_col} ~ C({factor_col})', data=df).fit()\n",
    "    anova_table = anova_lm(model, typ=2)\n",
    "\n",
    "    return {\n",
    "        'levene': (w_stat, p_levene),\n",
    "        'method': method,\n",
    "        'stat': stat,\n",
    "        'p_value': pval,\n",
    "        'tukey': tukey,\n",
    "        'anova_table': anova_table\n",
    "    }\n",
    "\n",
    "def interpret_duration_results(results: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate an English interpretation of the statistical test results\n",
    "    dictionary returned by `test_duration_by_format`.\n",
    "    \"\"\"\n",
    "    w_stat, p_levene = results['levene']\n",
    "    method = results['method']\n",
    "    stat = results['stat']\n",
    "    pval = results['p_value']\n",
    "\n",
    "    lines = []\n",
    "    # Interpret Levene's test\n",
    "    if p_levene > 0.05:\n",
    "        lines.append(\n",
    "            f\"Levene's test for homogeneity of variances: W = {w_stat:.3f}, \"\n",
    "            f\"p = {p_levene:.3f} (> 0.05). Variances are not significantly different.\\n\"\n",
    "        )\n",
    "    else:\n",
    "        lines.append(\n",
    "            f\"Levene's test for homogeneity of variances: W = {w_stat:.3f}, \"\n",
    "            f\"p = {p_levene:.3f} (≤ 0.05). Variances differ significantly; \"\n",
    "            f\"using {method}.\\n\"\n",
    "        )\n",
    "\n",
    "    # Interpret main test\n",
    "    if method == 'ANOVA':\n",
    "        lines.append(f\"One-way ANOVA: F = {stat:.3f}, p = {pval:.3f}.\\n\")\n",
    "        if pval < 0.05:\n",
    "            lines.append(\n",
    "                \"→ p < 0.05: Reject the null hypothesis of equal means; \"\n",
    "                \"there is a significant difference between groups.\\n\"\n",
    "            )\n",
    "            lines.append(\n",
    "                \"  Check the Tukey HSD results to see which pairs differ.\\n\"\n",
    "            )\n",
    "        else:\n",
    "            lines.append(\n",
    "                \"→ p ≥ 0.05: Cannot reject the null hypothesis; \"\n",
    "                \"no significant difference in means between groups.\\n\"\n",
    "            )\n",
    "    else:\n",
    "        lines.append(f\"Kruskal–Wallis H = {stat:.3f}, p = {pval:.3f}.\\n\")\n",
    "        if pval < 0.05:\n",
    "            lines.append(\n",
    "                \"→ p < 0.05: Reject the null hypothesis; \"\n",
    "                \"there is a significant difference between groups.\\n\"\n",
    "            )\n",
    "        else:\n",
    "            lines.append(\n",
    "                \"→ p ≥ 0.05: Cannot reject the null hypothesis; \"\n",
    "                \"no significant difference between groups.\\n\"\n",
    "            )\n",
    "\n",
    "    return \"\".join(lines)\n",
    "\n",
    "def extract_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    value_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identify outliers in `value_col` within each group defined by `group_col`\n",
    "    using the 1.5 × IQR rule. Returns a DataFrame containing only the outlier rows,\n",
    "    with additional columns:\n",
    "      - outlier_group: the group name\n",
    "      - lower_bound: lower threshold used\n",
    "      - upper_bound: upper threshold used\n",
    "    \"\"\"\n",
    "    outliers_list = []\n",
    "\n",
    "    for group_name, group_df in df.groupby(group_col):\n",
    "        q1 = group_df[value_col].quantile(0.25)\n",
    "        q3 = group_df[value_col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "        mask = (group_df[value_col] < lower_bound) | (group_df[value_col] > upper_bound)\n",
    "        outlier_rows = group_df.loc[mask].copy()\n",
    "        if not outlier_rows.empty:\n",
    "            outlier_rows['outlier_group'] = group_name\n",
    "            outlier_rows['lower_bound'] = lower_bound\n",
    "            outlier_rows['upper_bound'] = upper_bound\n",
    "            outliers_list.append(outlier_rows)\n",
    "\n",
    "    if outliers_list:\n",
    "        return pd.concat(outliers_list, ignore_index=True)\n",
    "    else:\n",
    "        # Return an empty DataFrame with the same structure\n",
    "        cols = list(df.columns) + ['outlier_group', 'lower_bound', 'upper_bound']\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "# ========== Example Usage ==========\n",
    "\n",
    "# Assume df_participant is already loaded and contains:\n",
    "# ['participantId', 'format', 'total_duration_min']\n",
    "\n",
    "# Run the statistical tests\n",
    "results = test_duration_by_format(\n",
    "    df_participant,\n",
    "    duration_col='total_duration_min',\n",
    "    factor_col='format'\n",
    ")\n",
    "\n",
    "print(\"\\nStatsmodels ANOVA table:\")\n",
    "print(results['anova_table'])\n",
    "\n",
    "print(\"\\nresult interpretation: \")\n",
    "print(interpret_duration_results(results))\n",
    "\n",
    "outliers = extract_outliers(df_participant, 'format', 'total_duration_sec')\n",
    "\n",
    "\n",
    "sns.boxplot(x='format', y='total_duration_sec', data=df_participant)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOutliers detected:\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from scipy.stats import levene, f_oneway, kruskal\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "def process_task_data(df_task: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and aggregate task-level duration data.\n",
    "\n",
    "    Steps:\n",
    "      1. Keep only relevant columns.\n",
    "      2. Remove tasks ending with '_post-task-question' and 'unqualified'.\n",
    "      3. Preserve original task rows (task_group = task).\n",
    "      4. Aggregate all 'modifying-', 'reading-', and 'writing-task-tabular-' subtasks\n",
    "         into their parent task_group by summing duration_min.\n",
    "      5. Aggregate all 'tutorial-<format>-partX' subtasks into 'tutorial-<format>' \n",
    "         by summing duration_min.\n",
    "    Returns a DataFrame with columns:\n",
    "      ['participantId', 'task_group', 'format', 'duration_min']\n",
    "    \"\"\"\n",
    "    # Select and filter relevant rows\n",
    "    df = df_task[['participantId', 'task', 'duration_min', 'format']].copy()\n",
    "    df = df[~df['task'].str.endswith('_post-task-question')]\n",
    "    df = df[df['task'] != 'unqualified']\n",
    "    \n",
    "    # 1) Preserve original rows: task_group = original task name\n",
    "    df_orig = df.rename(columns={'task': 'task_group'})[\n",
    "        ['participantId', 'task_group', 'format', 'duration_min']\n",
    "    ]\n",
    "    \n",
    "    agg_frames = []\n",
    "    # 2) Aggregate tabular subtasks into top-level task_group\n",
    "    for prefix in ['modifying-task-tabular', 'reading-task-tabular', 'writing-task-tabular']:\n",
    "        mask = df['task'].str.startswith(prefix + '-')\n",
    "        if mask.any():\n",
    "            tmp = (\n",
    "                df[mask]\n",
    "                .groupby(['participantId', 'format'], as_index=False)['duration_min']\n",
    "                .sum()\n",
    "            )\n",
    "            tmp['task_group'] = prefix\n",
    "            agg_frames.append(tmp[['participantId', 'task_group', 'format', 'duration_min']])\n",
    "    \n",
    "    # 3) Aggregate tutorial subtasks into 'tutorial-<format>'\n",
    "    tutorial_mask = df['task'].str.match(r'^tutorial-[^-]+-part\\d+$')\n",
    "    if tutorial_mask.any():\n",
    "        df_tut = df[tutorial_mask].copy()\n",
    "        df_tut['task_group'] = df_tut['task'].str.replace(r'-part\\d+$', '', regex=True)\n",
    "        tmp2 = (\n",
    "            df_tut\n",
    "            .groupby(['participantId', 'format', 'task_group'], as_index=False)['duration_min']\n",
    "            .sum()\n",
    "        )\n",
    "        agg_frames.append(tmp2[['participantId', 'task_group', 'format', 'duration_min']])\n",
    "    \n",
    "    # Combine original and aggregated rows\n",
    "    df_combined = pd.concat([df_orig] + agg_frames, ignore_index=True)\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def test_format_duration(df_combined: pd.DataFrame, task_list: list):\n",
    "    \"\"\"\n",
    "    For each task in task_list (raw or aggregated name):\n",
    "      1. Canonicalize the task name (e.g., 'modifying-task-tabular-1' → 'modifying-task-tabular').\n",
    "      2. Ensure there are at least two formats; skip otherwise.\n",
    "      3. Perform Levene's test for variance homogeneity.\n",
    "      4. If variances are homogeneous, run one-way ANOVA; else run Kruskal–Wallis.\n",
    "      5. If ANOVA was used, perform Tukey HSD post-hoc comparisons.\n",
    "      6. Print results and return a dictionary of statistics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for raw_task in task_list:\n",
    "        # Map raw_task to its canonical aggregated name\n",
    "        canonical = raw_task\n",
    "        for prefix in ['modifying-task-tabular', 'reading-task-tabular', 'writing-task-tabular']:\n",
    "            if raw_task.startswith(prefix + '-'):\n",
    "                canonical = prefix\n",
    "                break\n",
    "        else:\n",
    "            if re.match(r'^tutorial-[^-]+-part\\d+$', raw_task):\n",
    "                canonical = re.sub(r'-part\\d+$', '', raw_task)\n",
    "        \n",
    "        sub = df_combined[df_combined['task_group'] == canonical]\n",
    "        if sub.empty:\n",
    "            print(f\"⚠️ No data found for task '{raw_task}' (canonical '{canonical}'), skipping.\")\n",
    "            continue\n",
    "        \n",
    "        formats = sub['format'].unique()\n",
    "        if len(formats) < 2:\n",
    "            print(f\"⚠️ Task '{raw_task}' (canonical '{canonical}') has only one format: {formats.tolist()}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare groups of duration values\n",
    "        groups = [group['duration_min'].values for _, group in sub.groupby('format')]\n",
    "        \n",
    "        # 1) Levene's test\n",
    "        w_stat, p_levene = levene(*groups)\n",
    "        \n",
    "        # 2) Choose test based on variance homogeneity\n",
    "        if p_levene > 0.05:\n",
    "            stat, pval = f_oneway(*groups)\n",
    "            method = 'ANOVA'\n",
    "        else:\n",
    "            stat, pval = kruskal(*groups)\n",
    "            method = 'Kruskal–Wallis'\n",
    "        \n",
    "        # 3) Tukey HSD if ANOVA\n",
    "        tukey = None\n",
    "        if method == 'ANOVA':\n",
    "            tukey = pairwise_tukeyhsd(\n",
    "                endog=sub['duration_min'],\n",
    "                groups=sub['format'],\n",
    "                alpha=0.05\n",
    "            )\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n=== Task: {raw_task} → Aggregated as: {canonical} ===\")\n",
    "        print(f\"Levene's test: W = {w_stat:.3f}, p = {p_levene:.3f}\")\n",
    "        if method == 'ANOVA':\n",
    "            print(f\"One-way ANOVA: F = {stat:.3f}, p = {pval:.3f}\")\n",
    "            print(tukey)\n",
    "        else:\n",
    "            print(f\"Kruskal–Wallis: H = {stat:.3f}, p = {pval:.3f}\")\n",
    "        print('-' * 40)\n",
    "        \n",
    "        results[raw_task] = {\n",
    "            'canonical': canonical,\n",
    "            'method': method,\n",
    "            'stat': stat,\n",
    "            'p_value': pval,\n",
    "            'levene': (w_stat, p_levene),\n",
    "            'tukey': tukey\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_interpretation(results: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Given the output of test_format_duration, automatically generate\n",
    "    an English interpretation for each task.\n",
    "    Returns a dict mapping raw_task to its interpretation string.\n",
    "    \"\"\"\n",
    "    interpretations = {}\n",
    "    for raw_task, info in results.items():\n",
    "        canonical = info['canonical']\n",
    "        w_stat, p_levene = info['levene']\n",
    "        method = info['method']\n",
    "        stat = info['stat']\n",
    "        pval = info['p_value']\n",
    "        \n",
    "        lines = [f\"=== Task: '{raw_task}' (aggregated as '{canonical}') ===\\n\"]\n",
    "        \n",
    "        # Interpret Levene's test\n",
    "        if p_levene > 0.05:\n",
    "            lines.append(\n",
    "                f\"Levene's test p = {p_levene:.3f} (> 0.05): \"\n",
    "                \"variances are homogeneous across formats.\\n\"\n",
    "            )\n",
    "        else:\n",
    "            lines.append(\n",
    "                f\"Levene's test p = {p_levene:.3f} (≤ 0.05): \"\n",
    "                \"variances are not homogeneous; proceeding with \"\n",
    "                f\"{method}.\\n\"\n",
    "            )\n",
    "        \n",
    "        # Interpret main test\n",
    "        if method == 'ANOVA':\n",
    "            lines.append(f\"One-way ANOVA result: F = {stat:.3f}, p = {pval:.3f}.\\n\")\n",
    "            if pval < 0.05:\n",
    "                lines.append(\n",
    "                    \"→ p < 0.05: Reject the null hypothesis of equal means; \"\n",
    "                    \"significant differences exist between formats.\\n\"\n",
    "                )\n",
    "                lines.append(\n",
    "                    \"  Check Tukey HSD post-hoc comparisons to identify which pairs differ.\\n\"\n",
    "                )\n",
    "            else:\n",
    "                lines.append(\n",
    "                    \"→ p ≥ 0.05: Cannot reject the null hypothesis; \"\n",
    "                    \"no significant differences in mean duration across formats.\\n\"\n",
    "                )\n",
    "        else:\n",
    "            lines.append(f\"Kruskal–Wallis result: H = {stat:.3f}, p = {pval:.3f}.\\n\")\n",
    "            if pval < 0.05:\n",
    "                lines.append(\n",
    "                    \"→ p < 0.05: Reject the null hypothesis; \"\n",
    "                    \"significant differences exist between formats.\\n\"\n",
    "                )\n",
    "            else:\n",
    "                lines.append(\n",
    "                    \"→ p ≥ 0.05: Cannot reject the null hypothesis; \"\n",
    "                    \"no significant differences between formats.\\n\"\n",
    "                )\n",
    "        \n",
    "        interpretations[raw_task] = \"\".join(lines)\n",
    "    return interpretations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_proc = process_task_data(df_task)\n",
    "df_proc['task_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    'modifying-task-tabular',\n",
    "    'modifying-task-tabular-1',\n",
    "    'modifying-task-tabular-2',\n",
    "    'modifying-task-tabular-3',\n",
    "    'modifying-task-tabular-4',\n",
    "    'reading-task-tabular',\n",
    "    'reading-task-tabular-1',\n",
    "    'reading-task-tabular-2',\n",
    "    'reading-task-tabular-3',\n",
    "    'reading-task-tabular-4',\n",
    "    'reading-task-tabular-5',\n",
    "    'writing-task-tabular',\n",
    "    'writing-task-NL'\n",
    "]\n",
    "res = test_format_duration(df_proc, tasks)\n",
    "interpretations = generate_interpretation(res)\n",
    "for raw, txt in interpretations.items():\n",
    "    print(txt)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
